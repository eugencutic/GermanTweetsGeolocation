{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "german_tweets_geolocation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGC7L7VVx4m5",
        "outputId": "cfd69c94-fa6b-43c9-dbc4-ba5b6340387c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6EAMDZkz3XW"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "import sklearn\n",
        "from sklearn import svm\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik4Iaqjttdsp"
      },
      "source": [
        "def load_data(data_path):\n",
        "  training_data = pd.read_csv(os.path.join(data_path, 'training.txt'),\n",
        "                            sep = ',', header=None)\n",
        "  test_data = pd.read_csv(os.path.join(data_path, 'test.txt'),\n",
        "                              sep = ',', header=None)\n",
        "  val_data = pd.read_csv(os.path.join(data_path, 'validation.txt'),\n",
        "                              sep = ',', header=None)\n",
        "  training_data.columns = ['Id', 'Latitude', 'Longitude', 'Tweet']\n",
        "  val_data.columns = ['Id', 'Latitude', 'Longitude', 'Tweet']\n",
        "  test_data.columns = ['Id', 'Tweet']\n",
        "  return training_data, val_data, test_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO8qLdHDzRLg"
      },
      "source": [
        "data_path = '/content/drive/MyDrive/German Tweets Geolocation/data/'\n",
        "training_data, val_data, test_data = load_data(data_path)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxQ8lUcYvuLT"
      },
      "source": [
        "training_ids = training_data['Id'].values\n",
        "training_latitudes = training_data['Latitude'].values\n",
        "training_longitudes = training_data['Longitude'].values\n",
        "training_tweets = training_data['Tweet'].values\n",
        "\n",
        "val_ids = val_data['Id'].values\n",
        "val_latitudes = val_data['Latitude'].values\n",
        "val_longitudes = val_data['Longitude'].values\n",
        "val_tweets = val_data['Tweet'].values\n",
        "\n",
        "test_ids = test_data['Id'].values\n",
        "test_tweets = test_data['Tweet'].values"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d6OFRYhwbZP",
        "outputId": "9688ab99-5614-470b-cda4-014f18664194"
      },
      "source": [
        "!pip install emot"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emot\n",
            "  Downloading https://files.pythonhosted.org/packages/49/07/20001ade19873de611b7b66a4d5e5aabbf190d65abea337d5deeaa2bc3de/emot-2.1-py3-none-any.whl\n",
            "Installing collected packages: emot\n",
            "Successfully installed emot-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd0-X4OtwUhI"
      },
      "source": [
        "# convert emojis and emoticons to words\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
        "def convert_emojis_and_emoticons_to_word(tweet):\n",
        "  for emote in UNICODE_EMO:\n",
        "    tweet = tweet.replace(emote, \n",
        "                          \" \" + \"_\".join(UNICODE_EMO[emote]\n",
        "                                  .replace(\",\",\"\")\n",
        "                                  .replace(\":\",\"\").split()) + \" \")\n",
        "  for emote in EMOTICONS:\n",
        "    tweet = re.sub(u'('+emote+')', \n",
        "                   \" \" + \"_\".join(EMOTICONS[emote]\n",
        "                            .replace(\",\",\"\")\n",
        "                            .split()) + \" \", tweet)\n",
        "    \n",
        "  return tweet\n",
        "\n",
        "\n",
        "training_tweets = [convert_emojis_and_emoticons_to_word(t) for t in training_tweets]\n",
        "val_tweets = [convert_emojis_and_emoticons_to_word(t) for t in val_tweets]\n",
        "test_tweets = [convert_emojis_and_emoticons_to_word(t) for t in test_tweets]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_Coh5tY-_y6"
      },
      "source": [
        "# convert emojis and emoticons to words\r\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\r\n",
        "def convert_emojis_and_emoticons_to_word(tweet):\r\n",
        "  for emote in UNICODE_EMO:\r\n",
        "    tweet = tweet.replace(emote, '')\r\n",
        "  for emote in EMOTICONS:\r\n",
        "    tweet = re.sub(u'('+emote+')', '', tweet)\r\n",
        "    \r\n",
        "  return tweet\r\n",
        "\r\n",
        "\r\n",
        "training_tweets = [convert_emojis_and_emoticons_to_word(t) for t in training_tweets]\r\n",
        "val_tweets = [convert_emojis_and_emoticons_to_word(t) for t in val_tweets]\r\n",
        "test_tweets = [convert_emojis_and_emoticons_to_word(t) for t in test_tweets]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICefW5vWvuN2"
      },
      "source": [
        "# to lower and remove punctuation\n",
        "training_tweets = [re.sub('[^\\w\\s]','', t.lower()) for t in training_tweets]\n",
        "val_tweets = [re.sub('[^\\w\\s]','', t.lower()) for t in val_tweets]\n",
        "test_tweets = [re.sub('[^\\w\\s]','', t.lower()) for t in test_tweets]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3quDtKCOky4",
        "outputId": "fa82bad6-446b-41e7-b148-0a76d3e8dc0f"
      },
      "source": [
        "!python -m spacy download de"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907057 sha256=42eb32f87062371c6238a440b30d12ceab6330b2f7a77c969499196734093ced\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z0_iqpq2/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knGolo2mOjzQ"
      },
      "source": [
        "# lemmatize\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('de')\n",
        "allowed = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
        "\n",
        "data_lemmatized = []\n",
        "for tweet in training_tweets:\n",
        "    data_lemmatized.append(\" \".join([token.lemma_ for token in nlp(tweet) if token.pos_ in allowed]))\n",
        "training_tweets = data_lemmatized\n",
        "\n",
        "data_lemmatized = []\n",
        "for tweet in val_tweets:\n",
        "    data_lemmatized.append(\" \".join([token.lemma_ for token in nlp(tweet) if token.pos_ in allowed]))\n",
        "val_tweets = data_lemmatized\n",
        "\n",
        "data_lemmatized = []\n",
        "for tweet in test_tweets:\n",
        "    data_lemmatized.append(\" \".join([token.lemma_ for token in nlp(tweet) if token.pos_ in allowed]))\n",
        "test_tweets = data_lemmatized"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS-LkwZjvuS6",
        "outputId": "c69790a7-954c-45ef-a81a-b702798d1afd"
      },
      "source": [
        "# eliminate stopwords\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words('german'))\n",
        "\n",
        "def eliminate_stopwords(tweet):\n",
        "  return \" \".join([word for word in tweet.split()\n",
        "                  if word not in stop_words])\n",
        "\n",
        "training_tweets = [eliminate_stopwords(tweet) for tweet in training_tweets]\n",
        "val_tweets = [eliminate_stopwords(tweet) for tweet in val_tweets]\n",
        "test_tweets = [eliminate_stopwords(tweet) for tweet in test_tweets]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bl89uF01INl"
      },
      "source": [
        "# remove most/least common\n",
        "from collections import Counter\n",
        "counter = Counter()\n",
        "for tweet in training_tweets:\n",
        "  for word in tweet.split():\n",
        "    counter[word] += 1\n",
        "\n",
        "most_common = set([word for (word, count) in counter.most_common(10)])\n",
        "n = len(counter.most_common())\n",
        "least_common = set([word for (word, count) in counter.most_common()[:n-9:-1]])\n",
        "\n",
        "def remove_most_least_common(tweet):\n",
        "  return \" \".join([word for word in tweet.split()\n",
        "                  if word not in most_common and word not in least_common])\n",
        "\n",
        "training_tweets = [remove_most_least_common(tweet) for tweet in training_tweets]\n",
        "val_tweets = [remove_most_least_common(tweet) for tweet in val_tweets]\n",
        "test_tweets = [remove_most_least_common(tweet) for tweet in test_tweets]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NpBrCR7B5uu"
      },
      "source": [
        "# replace empty strings with \"EMPTY\"\n",
        "for i in range(len(training_tweets)):\n",
        "  if not training_tweets[i]:\n",
        "    training_tweets[i] = 'EMPTY'\n",
        "\n",
        "for i in range(len(val_tweets)):\n",
        "  if not val_tweets[i]:\n",
        "    val_tweets[i] = 'EMPTY'\n",
        "\n",
        "for i in range(len(test_tweets)):\n",
        "  if not test_tweets[i]:\n",
        "    test_tweets[i] = 'EMPTY'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DO_vsvt22sD"
      },
      "source": [
        "train_preprocessed = pd.DataFrame(columns=['Id', 'Latitude', 'Longitude', 'Tweet'])\n",
        "train_preprocessed['Id'] = training_ids\n",
        "train_preprocessed['Latitude'] = training_latitudes\n",
        "train_preprocessed['Longitude'] = training_longitudes\n",
        "train_preprocessed['Tweet'] = training_tweets\n",
        "\n",
        "val_preprocessed = pd.DataFrame(columns=['Id', 'Latitude', 'Longitude', 'Tweet'])\n",
        "val_preprocessed['Id'] = val_ids\n",
        "val_preprocessed['Latitude'] = val_latitudes\n",
        "val_preprocessed['Longitude'] = val_longitudes\n",
        "val_preprocessed['Tweet'] = val_tweets\n",
        "\n",
        "test_preprocessed = pd.DataFrame(columns=['Id', 'Tweet'])\n",
        "test_preprocessed['Id'] = test_ids\n",
        "test_preprocessed['Tweet'] = test_tweets"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqSwe5W43rCt"
      },
      "source": [
        "train_preprocessed.to_csv(\"/content/drive/My Drive/German Tweets Geolocation/data/train_preprocessed_no_emojis.txt\", index=False)\n",
        "val_preprocessed.to_csv(\"/content/drive/My Drive/German Tweets Geolocation/data/val_preprocessed_no_emojis.txt\", index=False)\n",
        "test_preprocessed.to_csv(\"/content/drive/My Drive/German Tweets Geolocation/data/test_preprocessed_no_emojis.txt\", index=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckOfRWHNS3Le",
        "outputId": "2f1e9daf-6528-4b8f-bc26-91b1bf516747"
      },
      "source": [
        "list(set(training_tweets[0].split()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dadruf',\n",
              " 'satomkraftwerk',\n",
              " 'anere',\n",
              " 'fett',\n",
              " 'seisch',\n",
              " 'antworten',\n",
              " 'bringen',\n",
              " 'gang',\n",
              " 'leben',\n",
              " 'langen',\n",
              " 'lache',\n",
              " 'schlecht',\n",
              " 'jahr',\n",
              " 'fangen',\n",
              " 'lebsch',\n",
              " 'nüt',\n",
              " 'frau',\n",
              " 'fall',\n",
              " 'kaputt',\n",
              " 'gar',\n",
              " 'unglück']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FByCLF3Fh1bn"
      },
      "source": [
        "def get_grams_util(alphabet, n, grams):\n",
        "    if n == 0:\n",
        "        grams[0] = []\n",
        "        return []\n",
        "    if n == 1:\n",
        "        grams[1] = list(alphabet)\n",
        "        return list(alphabet)\n",
        "    \n",
        "    if len(grams[n - 1]) > 0:\n",
        "        prev_grams = grams[n - 1]\n",
        "    else:\n",
        "        prev_grams = get_grams_util(alphabet, n - 1, grams)\n",
        "\n",
        "    gram = []\n",
        "    for letter in alphabet:\n",
        "        for string in prev_grams:\n",
        "            gram.append(letter + string)\n",
        "    \n",
        "    grams[n] = gram\n",
        "    return gram\n",
        "\n",
        "\n",
        "def get_grams(alphabet, n):\n",
        "    grams = [[] for _ in range(n + 1)]\n",
        "    get_grams_util(alphabet, n, grams)\n",
        "    return grams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxRHt0j0SFty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3a6e402-7e45-4f39-bd15-3eb43269d8f0"
      },
      "source": [
        "# trial\n",
        "words = list(set(training_tweets[0].split()))[:5]\n",
        "\n",
        "kernel = np.zeros((len(words), len(words)))\n",
        "\n",
        "for i in range(kernel.shape[0] // 2 + 1):\n",
        "  for j in range(kernel.shape[1] // 2 + 1):\n",
        "    alphabet = list(set(words[i]) | set(words[j]))\n",
        "    all_grams = get_grams(alphabet, 2)\n",
        "    for n_gram in all_grams:\n",
        "      for gram in n_gram:\n",
        "        if gram in words[i] and gram in words[j]:\n",
        "          kernel[i, j] += 1\n",
        "          kernel[j, i] += 1\n",
        "\n",
        "print(kernel)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[20.  6.  4.  0.  0.]\n",
            " [ 6. 46.  8.  0.  0.]\n",
            " [ 4.  8. 16.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXarf-RiFaJQ"
      },
      "source": [
        "# TF IDF\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "vectorizer = TfidfVectorizer()\r\n",
        "vectorizer.fit(training_tweets)\r\n",
        "\r\n",
        "train_sequences = vectorizer.transform(training_tweets)\r\n",
        "val_sequences = vectorizer.transform(val_tweets)\r\n",
        "test_sequences = vectorizer.transform(test_tweets)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21dVkTxVfeo6"
      },
      "source": [
        "# tokenize\n",
        "tokenizer = Tokenizer(num_words=None, char_level=False, oov_token='OOV')\n",
        "tokenizer.fit_on_texts(training_tweets)\n",
        "\n",
        "# get sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(training_tweets)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_tweets)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_tweets)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "AARMRq4hlZyt",
        "outputId": "24129bf0-de09-4f6e-cb22-38f77cd77390"
      },
      "source": [
        "max1, max2, max3 = max([len(s) for s in train_sequences]), max([len(s) for s in val_sequences]), max([len(s) for s in test_sequences])\n",
        "max_len = max(max1, max2, max3)\n",
        "print(max_len)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-6cb3971b1abb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmax1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_sequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_sequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-6cb3971b1abb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmax1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_sequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_sequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;31m# non-zeros is more important.  For now, raise an exception!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n\u001b[0m\u001b[1;32m    296\u001b[0m                         \" or shape[0]\")\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sparse matrix length is ambiguous; use getnnz() or shape[0]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTUgc7K1lTCw"
      },
      "source": [
        "# pad sequences\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_len, padding='post')\n",
        "val_sequences = pad_sequences(val_sequences, maxlen=max_len, padding='post')\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_len, padding='post')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYOVXXGdCJSJ"
      },
      "source": [
        "train_sequences = np.array(train_sequences)\n",
        "training_latitudes = np.array(training_latitudes)\n",
        "training_longitudes = np.array(training_longitudes)\n",
        "\n",
        "# labels for multioutput model\n",
        "train_labels = np.zeros((training_latitudes.shape[0], 2))\n",
        "train_labels[:, 0] = np.array(training_latitudes)\n",
        "train_labels[:, 1] = np.array(training_longitudes)\n",
        "\n",
        "val_sequences = np.array(val_sequences)\n",
        "val_latitudes = np.array(val_latitudes)\n",
        "val_longitudes = np.array(val_longitudes)\n",
        "\n",
        "#labels for multioutput models\n",
        "val_labels = np.zeros((val_latitudes.shape[0], 2))\n",
        "val_labels[:, 0] = np.array(val_latitudes)\n",
        "val_labels[:, 1] = np.array(val_longitudes)\n",
        "\n",
        "test_sequences = np.array(test_sequences)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfcnNDEsod6V"
      },
      "source": [
        "# construct grid search\n",
        "from sklearn import metrics\n",
        "\n",
        "nus = [0.1, 0.3, 0.5, 0.7, 1]\n",
        "Cs = [0.1, 1, 10, 100]\n",
        "parameters = [{'C':Cs, 'nu': nus}]\n",
        "\n",
        "scorer = metrics.make_scorer(metrics.mean_squared_error, \n",
        "                             greater_is_better=False)\n",
        "\n",
        "svr_lat_grid = svr_long_grid = sklearn.model_selection.GridSearchCV(svm.NuSVR(), \n",
        "                                           parameters, \n",
        "                                           cv = 20,\n",
        "                                           scoring=scorer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XV4JP9QoeDG"
      },
      "source": [
        "# execute grid search for latitudes\n",
        "svr_lat_grid.fit(train_sequences, training_latitudes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "8IXWB-ezex7s",
        "outputId": "5feda4eb-266e-4f66-9d48-23032bada14f"
      },
      "source": [
        "svr_lat_grid.cv_results_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1d7829d657d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msvr_lat_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'svr_lat_grid' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lpMaRe-I_fY"
      },
      "source": [
        "# execute grid search for longitudes\n",
        "svr_long_grid.fit(traing_sequences, training_longitudes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB1r7OPxoeH1",
        "outputId": "8b7ab57c-eba7-476b-d727-c8c443119dc0"
      },
      "source": [
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "multi_regressor_test = MultiOutputRegressor(svm.NuSVR(verbose=True))\n",
        "multi_regressor_test.fit(train_sequences, train_labels)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM][LibSVM]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiOutputRegressor(estimator=NuSVR(C=1.0, cache_size=200, coef0=0.0, degree=3,\n",
              "                                     gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                                     nu=0.5, shrinking=True, tol=0.001,\n",
              "                                     verbose=True),\n",
              "                     n_jobs=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE2XGFfB3-JQ",
        "outputId": "27a56b8e-6962-470e-93be-ae3ab1056d8b"
      },
      "source": [
        "from sklearn import metrics\n",
        "predictions = multi_regressor_test.predict(val_sequences)\n",
        "mse_1 = metrics.mean_squared_error(val_labels[:, 0], predictions[:, 0]) \n",
        "mse_2 = metrics.mean_squared_error(val_labels[:, 1], predictions[:, 1])\n",
        "print(mse_1)\n",
        "print(mse_2)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5865341952682823\n",
            "1.1069406605915566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHRMQQAMraKn"
      },
      "source": [
        "test_predictions = multi_regressor_test.predict(test_sequences)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oYuoenQrsFw"
      },
      "source": [
        "submission_df = pd.DataFrame(columns=[\"id\", \"lat\", \"long\"])\n",
        "submission_df['id'] = test_ids\n",
        "submission_df['lat'] = test_predictions[:, 0]\n",
        "submission_df['long'] = test_predictions[:, 1]\n",
        "submission_df.to_csv(\"/content/drive/My Drive/German Tweets Geolocation/nu_svr_multioutput_submission_tfidf.txt\", index=False)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXcdJT30NCn6"
      },
      "source": [
        "# initialize embeddings\n",
        "embeddings = []\n",
        "embeddings.append(np.zeros(vocab_size)) # one-hot for OOV token\n",
        "for char, idx in tokenizer.word_index.items():\n",
        "  char_onehot = np.zeros(vocab_size)\n",
        "  char_onehot[idx - 1] # idx-1 because word_index indexes from 1\n",
        "  embeddings.append(char_onehot)\n",
        "\n",
        "embeddings = np.array(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGyZQ5zOPnHz"
      },
      "source": [
        "# construct the model \n",
        "embedding_size = vocab_size\n",
        "input_size = 750\n",
        "\n",
        "model = keras.Sequential([\n",
        "      layers.Embedding(vocab_size + 1, embedding_size, \n",
        "                      input_length=input_size, weights = [embeddings]),\n",
        "      layers.Conv1D(256, 7, activation=keras.activations.relu),\n",
        "      layers.MaxPooling1D(3),\n",
        "      layers.Conv1D(256, 7, activation=keras.activations.relu),\n",
        "      layers.MaxPooling1D(3),\n",
        "      layers.Conv1D(256, 3, activation=keras.activations.relu),\n",
        "      layers.Conv1D(256, 3, activation=keras.activations.relu),\n",
        "      layers.Conv1D(256, 3, activation=keras.activations.relu),\n",
        "      layers.Conv1D(256, 3, activation=keras.activations.relu),\n",
        "      layers.GlobalMaxPooling1D(),\n",
        "      layers.Dense(1024, activation=keras.activations.relu),\n",
        "      layers.Dropout(0.5),\n",
        "      layers.Dense(1024, activation=keras.activations.relu),\n",
        "      layers.Dropout(0.5),\n",
        "      layers.Dense(2)\n",
        "  ]\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=10e-3)\n",
        "loss = keras.losses.MeanSquaredError()\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrS_7oZ0Q6n8",
        "outputId": "5047b7b3-bcea-440f-b411-4eb7a46641b1"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 750, 1008)         1017072   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 744, 256)          1806592   \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 248, 256)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 242, 256)          459008    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 80, 256)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 78, 256)           196864    \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 76, 256)           196864    \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 74, 256)           196864    \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 72, 256)           196864    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              263168    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 2050      \n",
            "=================================================================\n",
            "Total params: 5,384,946\n",
            "Trainable params: 5,384,946\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWmcg17SVS4N"
      },
      "source": [
        "y_train_shape = (training_latitudes.shape[0], 2)\n",
        "\n",
        "x_train = train_sequences\n",
        "y_train = np.zeros(y_train_shape)\n",
        "y_train[:, 0] = training_latitudes\n",
        "y_train[:, 1] = training_longitudes\n",
        "\n",
        "x_val = val_sequences\n",
        "y_val_shape = (val_latitudes.shape[0], 2)\n",
        "y_val = np.zeros(y_val_shape)\n",
        "y_val[:, 0] = val_latitudes\n",
        "y_val[:, 1] = val_longitudes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_arHq1AYXXEO",
        "outputId": "4814e7fe-eb01-4ceb-af48-56bc5670aaf1"
      },
      "source": [
        "model.fit(x_train, y_train, validation_split=0.3,\n",
        "          batch_size=128, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "124/124 [==============================] - 36s 287ms/step - loss: 1050.4204 - accuracy: 1.0000 - val_loss: 1024.1136 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "124/124 [==============================] - 35s 285ms/step - loss: 995.0497 - accuracy: 1.0000 - val_loss: 970.0140 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "124/124 [==============================] - 35s 286ms/step - loss: 942.4090 - accuracy: 1.0000 - val_loss: 918.5295 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "124/124 [==============================] - 35s 285ms/step - loss: 892.2535 - accuracy: 1.0000 - val_loss: 869.4316 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "124/124 [==============================] - 35s 286ms/step - loss: 844.3678 - accuracy: 1.0000 - val_loss: 822.5183 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "124/124 [==============================] - 35s 285ms/step - loss: 798.5679 - accuracy: 1.0000 - val_loss: 777.6205 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "124/124 [==============================] - 35s 286ms/step - loss: 754.6976 - accuracy: 1.0000 - val_loss: 734.5930 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "124/124 [==============================] - 35s 284ms/step - loss: 712.6282 - accuracy: 1.0000 - val_loss: 693.3246 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "124/124 [==============================] - 35s 284ms/step - loss: 672.2587 - accuracy: 1.0000 - val_loss: 653.7172 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "124/124 [==============================] - 35s 284ms/step - loss: 633.5071 - accuracy: 1.0000 - val_loss: 615.7025 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f29b65e4e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W73DQ2YGXt6M"
      },
      "source": [
        "predictions = model.predict(x_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vcb4mDuLvpFn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}