# -*- coding: utf-8 -*-
"""german_tweets_geolocation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jkYcbiVygk2XBSrBe0WrjswZ8GBB5Hp8?usp=sharing
"""

# needed only on Google Colab
# from google.colab import drive
# drive.mount('/content/drive')

import os
import sys

import pandas as pd
import numpy as np

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

import keras
from keras import layers

import sklearn
from sklearn import svm

import re
import nltk
import spacy
import string

def load_data(data_path):
  training_data = pd.read_csv(os.path.join(data_path, 'training.txt'),
                            sep = ',', header=None)
  test_data = pd.read_csv(os.path.join(data_path, 'test.txt'),
                              sep = ',', header=None)
  val_data = pd.read_csv(os.path.join(data_path, 'validation.txt'),
                              sep = ',', header=None)
  training_data.columns = ['Id', 'Latitude', 'Longitude', 'Tweet']
  val_data.columns = ['Id', 'Latitude', 'Longitude', 'Tweet']
  test_data.columns = ['Id', 'Tweet']
  return training_data, val_data, test_data

# data path holding reference to the Google drive data folder
data_path = '/content/drive/MyDrive/German Tweets Geolocation/data/'
training_data, val_data, test_data = load_data(data_path)

training_ids = training_data['Id'].values
training_latitudes = training_data['Latitude'].values
training_longitudes = training_data['Longitude'].values
training_tweets = training_data['Tweet'].values.astype(str)

val_ids = val_data['Id'].values
val_latitudes = val_data['Latitude'].values
val_longitudes = val_data['Longitude'].values
val_tweets = val_data['Tweet'].values.astype(str)

test_ids = test_data['Id'].values
test_tweets = test_data['Tweet'].values.astype(str)


# function to remove any special character like emoticons, symbols or chinese characters (which appear in the data)
def remove_special_chars(text):
    text = text.replace('_', '')
    char_pattern = re.compile("["
                                u"\U0001F600-\U0001F64F"  # emoticons
                                u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                                u"\U0001F680-\U0001F6FF"  # transport & map symbols
                                u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                                u"\U00002500-\U00002BEF"  # chinese char
                                u"\U00002702-\U000027B0"
                                u"\U00002702-\U000027B0"
                                u"\U000024C2-\U0001F251"
                                u"\U0001f926-\U0001f937"
                                u"\U00010000-\U0010ffff"
                                u"\u2640-\u2642"
                                u"\u2600-\u2B55"
                                u"\u200d"
                                u"\u23cf"
                                u"\u23e9"
                                u"\u231a"
                                u"\ufe0f"  # dingbats
                                u"\u3030"
                                "]+", flags=re.UNICODE)
    return re.sub(char_pattern, '', text)


training_tweets = [remove_special_chars(t) for t in training_tweets]
val_tweets = [remove_special_chars(t) for t in val_tweets]
test_tweets = [remove_special_chars(t) for t in test_tweets]


# Remove digits
def remove_digits(text):
  return ''.join([c for c in text if not c.isdigit()])

training_tweets = [remove_digits(t) for t in training_tweets]
val_tweets = [remove_digits(t) for t in val_tweets]
test_tweets = [remove_digits(t) for t in test_tweets]


# needed only on Google Colab
# !pip install emot

# as this was run as a Notebook it gave a choice as to whether one whishes to remove 
# emojis completely, or convert them into meaningful description (e.g. 'smiley_face')
from emot.emo_unicode import UNICODE_EMO, EMOTICONS
def convert_emojis_and_emoticons_to_word(tweet):
  for emote in UNICODE_EMO:
    tweet = tweet.replace(emote, 
                          " " + "_".join(UNICODE_EMO[emote]
                                  .replace(",","")
                                  .replace(":","").split()) + " ")
  for emote in EMOTICONS:
    tweet = re.sub(u'('+emote+')', 
                   " " + "_".join(EMOTICONS[emote]
                            .replace(",","")
                            .split()) + " ", tweet)
    
  return tweet


###
# The following code has been commented out because of the reason stated above.
# It represented different cells in a notebook and, given this .py file removes
# all emojis before this line, this is no longer needed
###

###

# training_tweets = [convert_emojis_and_emoticons_to_word(t) for t in training_tweets]
# val_tweets = [convert_emojis_and_emoticons_to_word(t) for t in val_tweets]
# test_tweets = [convert_emojis_and_emoticons_to_word(t) for t in test_tweets]

# # delete emojis and emoticons
# from emot.emo_unicode import UNICODE_EMO, EMOTICONS
# def convert_emojis_and_emoticons_to_word(tweet):
#   for emote in UNICODE_EMO:
#     tweet = tweet.replace(emote, '')
#   for emote in EMOTICONS:
#     tweet = re.sub(u'('+emote+')', '', tweet)
    
#   return tweet


# training_tweets = [convert_emojis_and_emoticons_to_word(t) for t in training_tweets]
# val_tweets = [convert_emojis_and_emoticons_to_word(t) for t in val_tweets]
# test_tweets = [convert_emojis_and_emoticons_to_word(t) for t in test_tweets]

###

# to lower and remove punctuation
training_tweets = [re.sub('[^\w\s]','', t.lower()) for t in training_tweets]
val_tweets = [re.sub('[^\w\s]','', t.lower()) for t in val_tweets]
test_tweets = [re.sub('[^\w\s]','', t.lower()) for t in test_tweets]

# needed only on Google Colab
# !python -m spacy download de

# lemmatize - keeping the roots of the words 
# and dropping any word which is not a noun, adjective, verb or adverb
import spacy

nlp = spacy.load('de')
allowed = ['NOUN', 'ADJ', 'VERB', 'ADV']

data_lemmatized = []
for tweet in training_tweets:
    data_lemmatized.append(" ".join([token.lemma_ for token in nlp(tweet) if token.pos_ in allowed]))
training_tweets = data_lemmatized

data_lemmatized = []
for tweet in val_tweets:
    data_lemmatized.append(" ".join([token.lemma_ for token in nlp(tweet) if token.pos_ in allowed]))
val_tweets = data_lemmatized

data_lemmatized = []
for tweet in test_tweets:
    data_lemmatized.append(" ".join([token.lemma_ for token in nlp(tweet) if token.pos_ in allowed]))
test_tweets = data_lemmatized

# eliminate stopwords
from nltk.corpus import stopwords
nltk.download("stopwords")
stop_words = set(stopwords.words('german'))

def eliminate_stopwords(tweet):
  return " ".join([word for word in tweet.split()
                  if word not in stop_words])

training_tweets = [eliminate_stopwords(tweet) for tweet in training_tweets]
val_tweets = [eliminate_stopwords(tweet) for tweet in val_tweets]
test_tweets = [eliminate_stopwords(tweet) for tweet in test_tweets]

# remove most/least common
from collections import Counter
counter = Counter()
for tweet in training_tweets:
  for word in tweet.split():
    counter[word] += 1

most_common = set([word for (word, count) in counter.most_common(10)])
n = len(counter.most_common())
least_common = set([word for (word, count) in counter.most_common()[:n-9:-1]])

def remove_most_least_common(tweet):
  return " ".join([word for word in tweet.split()
                  if word not in most_common and word not in least_common])

training_tweets = [remove_most_least_common(tweet) for tweet in training_tweets]
val_tweets = [remove_most_least_common(tweet) for tweet in val_tweets]
test_tweets = [remove_most_least_common(tweet) for tweet in test_tweets]

# after removing stopwords and most and least common words some strings might end up empty
# the final results do not use this preprocessing method as it has not proven useful
# using the Colab Notebook it is not recommended to run the cell containing this code
# replace empty strings with "EMPTY"
for i in range(len(training_tweets)):
  if not training_tweets[i]:
    training_tweets[i] = 'EMPTY'

for i in range(len(val_tweets)):
  if not val_tweets[i]:
    val_tweets[i] = 'EMPTY'

for i in range(len(test_tweets)):
  if not test_tweets[i]:
    test_tweets[i] = 'EMPTY'


# save preprocessed data for ease of access during training
train_preprocessed = pd.DataFrame(columns=['Id', 'Latitude', 'Longitude', 'Tweet'])
train_preprocessed['Id'] = training_ids
train_preprocessed['Latitude'] = training_latitudes
train_preprocessed['Longitude'] = training_longitudes
train_preprocessed['Tweet'] = training_tweets

val_preprocessed = pd.DataFrame(columns=['Id', 'Latitude', 'Longitude', 'Tweet'])
val_preprocessed['Id'] = val_ids
val_preprocessed['Latitude'] = val_latitudes
val_preprocessed['Longitude'] = val_longitudes
val_preprocessed['Tweet'] = val_tweets

test_preprocessed = pd.DataFrame(columns=['Id', 'Tweet'])
test_preprocessed['Id'] = test_ids
test_preprocessed['Tweet'] = test_tweets

train_preprocessed.to_csv("/content/drive/My Drive/German Tweets Geolocation/data/train_preprocessed_no_emojis.txt", index=False)
val_preprocessed.to_csv("/content/drive/My Drive/German Tweets Geolocation/data/val_preprocessed_no_emojis.txt", index=False)
test_preprocessed.to_csv("/content/drive/My Drive/German Tweets Geolocation/data/test_preprocessed_no_emojis.txt", index=False)
